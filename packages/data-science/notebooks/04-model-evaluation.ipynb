{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Objective\n",
    "Comprehensive evaluation of trained models:\n",
    "- Confusion matrices\n",
    "- Per-class metrics\n",
    "- Error analysis\n",
    "- Confidence calibration\n",
    "- Model fairness (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import json\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "models_dir = Path('../models')\n",
    "with open(models_dir / 'category_model.pkl', 'rb') as f:\n",
    "    cat_model = pickle.load(f)\n",
    "with open(models_dir / 'priority_model.pkl', 'rb') as f:\n",
    "    pri_model = pickle.load(f)\n",
    "\n",
    "# Load encoders\n",
    "data_dir = Path('../data/processed')\n",
    "with open(data_dir / 'category_encoder.pkl', 'rb') as f:\n",
    "    category_encoder = pickle.load(f)\n",
    "with open(data_dir / 'priority_encoder.pkl', 'rb') as f:\n",
    "    priority_encoder = pickle.load(f)\n",
    "\n",
    "# Load test data (recreate split from training notebook)\n",
    "X = scipy.sparse.load_npz(data_dir / 'X_tfidf.npz')\n",
    "y_category = np.load(data_dir / 'y_category.npy')\n",
    "y_priority = np.load(data_dir / 'y_priority.npy')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_cat_train, y_cat_test, y_pri_train, y_pri_test = train_test_split(\n",
    "    X, y_category, y_priority, test_size=0.2, random_state=RANDOM_STATE, stratify=y_category\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Category Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_cat_pred = cat_model.predict(X_test)\n",
    "y_cat_proba = cat_model.predict_proba(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "cat_accuracy = accuracy_score(y_cat_test, y_cat_pred)\n",
    "print(f\"Category Model Accuracy: {cat_accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_cat_test, y_cat_pred, target_names=category_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_cat = confusion_matrix(y_cat_test, y_cat_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_cat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=category_encoder.classes_,\n",
    "            yticklabels=category_encoder.classes_)\n",
    "plt.title('Category Prediction Confusion Matrix')\n",
    "plt.ylabel('True Category')\n",
    "plt.xlabel('Predicted Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Priority Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pri_pred = pri_model.predict(X_test)\n",
    "y_pri_proba = pri_model.predict_proba(X_test)\n",
    "\n",
    "# Overall accuracy\n",
    "pri_accuracy = accuracy_score(y_pri_test, y_pri_pred)\n",
    "print(f\"Priority Model Accuracy: {pri_accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_pri_test, y_pri_pred, target_names=priority_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_pri = confusion_matrix(y_pri_test, y_pri_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_pri, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=priority_encoder.classes_,\n",
    "            yticklabels=priority_encoder.classes_)\n",
    "plt.title('Priority Prediction Confusion Matrix')\n",
    "plt.ylabel('True Priority')\n",
    "plt.xlabel('Predicted Priority')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load original text data to analyze misclassifications\n",
    "# Identify samples where model prediction was wrong\n",
    "# cat_errors = (y_cat_pred != y_cat_test)\n",
    "# print(f\"Category errors: {cat_errors.sum()} / {len(y_cat_test)}\")\n",
    "\n",
    "# Analyze error patterns:\n",
    "# - Which categories are most confused?\n",
    "# - Are low-confidence predictions more likely to be wrong?\n",
    "# - What text patterns lead to errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract max probability (confidence) for each prediction\n",
    "cat_confidence = y_cat_proba.max(axis=1)\n",
    "pri_confidence = y_pri_proba.max(axis=1)\n",
    "\n",
    "# Analyze confidence distribution\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(cat_confidence, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Category Confidence Distribution')\n",
    "plt.axvline(cat_confidence.mean(), color='red', linestyle='--', label=f'Mean: {cat_confidence.mean():.2f}')\n",
    "plt.axvline(0.6, color='green', linestyle='--', label='Threshold: 0.6')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(pri_confidence, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Priority Confidence Distribution')\n",
    "plt.axvline(pri_confidence.mean(), color='red', linestyle='--', label=f'Mean: {pri_confidence.mean():.2f}')\n",
    "plt.axvline(0.6, color='green', linestyle='--', label='Threshold: 0.6')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Percentage above threshold\n",
    "print(f\"Category predictions with confidence > 0.6: {(cat_confidence > 0.6).mean()*100:.1f}%\")\n",
    "print(f\"Priority predictions with confidence > 0.6: {(pri_confidence > 0.6).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy vs. Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if higher confidence correlates with higher accuracy\n",
    "confidence_thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "accuracies_cat = []\n",
    "coverage_cat = []\n",
    "\n",
    "for threshold in confidence_thresholds:\n",
    "    mask = cat_confidence >= threshold\n",
    "    if mask.sum() > 0:\n",
    "        acc = accuracy_score(y_cat_test[mask], y_cat_pred[mask])\n",
    "        cov = mask.mean()\n",
    "        accuracies_cat.append(acc)\n",
    "        coverage_cat.append(cov)\n",
    "    else:\n",
    "        accuracies_cat.append(0)\n",
    "        coverage_cat.append(0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(confidence_thresholds, accuracies_cat, marker='o', label='Accuracy')\n",
    "plt.plot(confidence_thresholds, coverage_cat, marker='s', label='Coverage')\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Category Model: Accuracy vs. Coverage by Confidence Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary metrics\n",
    "summary = {\n",
    "    \"category_model\": {\n",
    "        \"accuracy\": float(cat_accuracy),\n",
    "        \"mean_confidence\": float(cat_confidence.mean()),\n",
    "        \"predictions_above_0.6_confidence\": float((cat_confidence > 0.6).mean())\n",
    "    },\n",
    "    \"priority_model\": {\n",
    "        \"accuracy\": float(pri_accuracy),\n",
    "        \"mean_confidence\": float(pri_confidence.mean()),\n",
    "        \"predictions_above_0.6_confidence\": float((pri_confidence > 0.6).mean())\n",
    "    },\n",
    "    \"test_set_size\": int(len(y_cat_test)),\n",
    "    \"evaluation_date\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations\n",
    "\n",
    "Based on evaluation results:\n",
    "\n",
    "**TODO: Fill in after running evaluation**\n",
    "\n",
    "- **Confidence Threshold**: Recommend using 0.6 or higher to ensure quality predictions\n",
    "- **Model Improvements**: <!-- e.g., Collect more data for minority classes, try advanced models -->\n",
    "- **Production Monitoring**: Track prediction accuracy over time to detect drift\n",
    "- **Fallback Strategy**: For low-confidence predictions, route to human agent without suggestion\n",
    "\n",
    "**Next Steps**:\n",
    "1. Document findings in `docs/MODEL-CARD.md`\n",
    "2. Implement prediction API in `api/app.py`\n",
    "3. Set up monitoring dashboard\n",
    "4. Plan model retraining schedule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
