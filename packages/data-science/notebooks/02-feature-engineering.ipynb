{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Objective\n",
    "Transform raw text data into numerical features suitable for machine learning models.\n",
    "\n",
    "## Approach\n",
    "1. Text preprocessing (cleaning, normalization)\n",
    "2. Feature extraction (TF-IDF, embeddings)\n",
    "3. Combine text and metadata features\n",
    "4. Save feature matrices for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (run once)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from EDA\n",
    "data_path = Path('../../../contracts/mock-data/requests.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded {len(df)} requests\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "### Define Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text: lowercase, remove special chars, remove stopwords, lemmatize\n",
    "    \n",
    "    TODO: Adjust preprocessing steps based on experimentation\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keep letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and description\n",
    "df['combined_text'] = df['title'] + ' ' + df['description']\n",
    "\n",
    "# Preprocess\n",
    "print(\"Preprocessing text...\")\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Check samples\n",
    "print(\"\\nOriginal vs. Processed:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['combined_text'].iloc[i][:100]}\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "# TODO: Tune parameters (max_features, ngram_range, min_df, max_df)\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=1000,  # Limit to top 1000 features\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in < 2 documents\n",
    "    max_df=0.8  # Ignore terms that appear in > 80% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_tfidf = tfidf.fit_transform(df['processed_text'])\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Feature names (sample): {tfidf.get_feature_names_out()[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect top features by category\n",
    "# TODO: Use this to validate features make sense\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "for category in df['category'].unique():\n",
    "    print(f\"\\n=== Top features for {category} ===\")\n",
    "    category_indices = df[df['category'] == category].index\n",
    "    category_tfidf = X_tfidf[category_indices].mean(axis=0).A1\n",
    "    top_indices = category_tfidf.argsort()[-10:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "    print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Word Embeddings (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: If using word embeddings (Word2Vec, GloVe, or pre-trained transformers)\n",
    "# Example with sentence transformers:\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# X_embeddings = model.encode(df['combined_text'].tolist())\n",
    "# print(f\"Embedding shape: {X_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encode Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode category\n",
    "category_encoder = LabelEncoder()\n",
    "y_category = category_encoder.fit_transform(df['category'])\n",
    "print(f\"Category classes: {category_encoder.classes_}\")\n",
    "\n",
    "# Encode priority\n",
    "priority_encoder = LabelEncoder()\n",
    "y_priority = priority_encoder.fit_transform(df['priority'])\n",
    "print(f\"Priority classes: {priority_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data and Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save TF-IDF features\n",
    "import scipy.sparse\n",
    "scipy.sparse.save_npz(output_dir / 'X_tfidf.npz', X_tfidf)\n",
    "\n",
    "# Save target variables\n",
    "np.save(output_dir / 'y_category.npy', y_category)\n",
    "np.save(output_dir / 'y_priority.npy', y_priority)\n",
    "\n",
    "# Save encoders and vectorizer for later use\n",
    "with open(output_dir / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "    \n",
    "with open(output_dir / 'category_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(category_encoder, f)\n",
    "    \n",
    "with open(output_dir / 'priority_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(priority_encoder, f)\n",
    "\n",
    "print(\"\\nSaved processed data and encoders to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Features (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add metadata features if available\n",
    "# Examples:\n",
    "# - Title length\n",
    "# - Description length\n",
    "# - Time of day (hour)\n",
    "# - Day of week\n",
    "# - Presence of urgent keywords\n",
    "\n",
    "# df['title_length'] = df['title'].str.len()\n",
    "# df['desc_length'] = df['description'].str.len()\n",
    "# df['has_urgent'] = df['combined_text'].str.contains('urgent|asap|critical', case=False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Summary\n",
    "\n",
    "**Created Features**:\n",
    "- TF-IDF vectors (1000 features, unigrams + bigrams)\n",
    "- Encoded category labels\n",
    "- Encoded priority labels\n",
    "\n",
    "**Saved Artifacts**:\n",
    "- `X_tfidf.npz`: Feature matrix\n",
    "- `y_category.npy`: Category labels\n",
    "- `y_priority.npy`: Priority labels\n",
    "- `tfidf_vectorizer.pkl`: Fitted vectorizer for inference\n",
    "- `category_encoder.pkl`: Category label encoder\n",
    "- `priority_encoder.pkl`: Priority label encoder\n",
    "\n",
    "TODO: Document any additional features or preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "1. Proceed to `03-model-training.ipynb`\n",
    "2. Load processed features\n",
    "3. Split into train/test sets\n",
    "4. Train classification models\n",
    "5. Evaluate performance in `04-model-evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
